# PLANS

All I did in last two years:
- form `2017-10-25` to `2018-10-18`, 1 year, I done the ML class and ML+ class.
- from `2018-10-19` to `2019-11-05`, 1 year, I only done 3 projects(Titanic, House-Price, DigitReconize), starting 2 projects(ASHRA, Future-Sales-Predict) in kaggle.

In one and half years ago, I writed this:

> **Get a job about machine learning whatever how much job-pay is!!!**<br>
>> I am 26 old now, I have nothing by myself to proud. The house? my parent pay it, the job? it just so-so. I am even not better than my father on his 26 old, and I have University degree. So what I do in shenzhen from 2015 year until now. Everyone I know is working hard but me, I feel shame on myself. But good news is I still have time to find and learn how to be better. Someone will say I just follow the wind of machine learning, but I dont give a shit, it will be difficult, and job-pay will be lesser, but I will get it and do it well, I will show everyone I can do this well. Of cause first I must to finish the udacity machine learning lesson.
    
And now, I am 27, still do nothing, I am scare now, maybe this is a wrong way and I am not a hard-work person, maybe I should always be a software-engineer not a MLer, but I want to try last time, from now to `2020-05-01`, I will study harder and harder, follow my plan(kaggle projects, kaggle kernels, books and videos), I will give up if I can not get a ML job between `2020-03` and `2020-04`, please be harder and dont make yourself regretting, when you feel tired, you look around your friends(xing, qi, rui), everybody work hard and get something but me, I should work harder than them.

## TARGET

Get a job about `Time-Series`, `Sale-Predict` and `Machine Learning`.

## Timetable

- work day:
    - `9:00` ~ `10:00`: relax;
    - `10:00` ~ `12:30`: work;
    - `12:30` ~ `14:00`: lunch and take a nap;
    - `14:00` ~ `19:30`: work;
    - `19:30` ~ `21:00`: dinner and take a shower;
    - `21:00` ~ `24:00`: study;
- rest day:
    - `10:00` ~ `12:00`: relax;
    - `12:00` ~ `14:00`: breakfast and take a walk and shop for dinner;
    - `14:00` ~ `18:00`: study;
    - `18:00` ~ `20:00`: dinner and take a shower and maybe take a walk;
    - `20:00` ~ `24:00`: study;
    
## DETAIL

When you waiting for the code run, you best to do something else, like learn from kernels, read book, watch video, write note, etc.

## PLAN-corgi ![icon](https://img.shields.io/badge/doing-70%25-green)

cycle time: from `2019-11-05` to `2019-11-30`;

0. ![icon](https://img.shields.io/badge/doing|A-90%25-green) finish [future-price-predict project](https://www.kaggle.com/holoong9291/predict-future-sales) with the good kernel
    1. deathline: `2019-11-10`;
    2. now: fix and update with [the good kernel](https://www.kaggle.com/dlarionov/feature-engineering-xgboost);
    3. and: model stack, correct factor;
    4. last: up to Top10%;
1. ![icon](https://img.shields.io/badge/done-100%25-lightgrey) learn how to work with time series with TS kernels
    1. deathline: `2019-11-17`;
    2. now: start with https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle ;
    3. first(common): https://www.kaggle.com/thebrownviking20/everything-you-can-do-with-a-time-series ;
    4. second(ARIMA): https://www.kaggle.com/myonin/bitcoin-price-prediction-by-arima ;
2. ![icon](https://img.shields.io/badge/doing|A-35%25-green) another competitions
    - [NFL Big Data Bowl](https://www.kaggle.com/c/nfl-big-data-bowl-2020/data)
        1. deathline: `2019-11-28`;
        2. [github project link](https://github.com/NemoHoHaloAi/Competition/tree/master/kaggle/Topxxx-yyy-zzz-NFL-Big-Data-Bowl)
        3. [start with](https://www.kaggle.com/gertjac/regression-approach)
        4. 等待竞赛结束后整理总结分析，学到很多，但是不得不说的是差距依然很大；
    - [Corporación Favorita Grocery Sales Forecasting](https://www.kaggle.com/c/favorita-grocery-sales-forecasting)
        1. deathline: `2019-12-15`;
        2. [github project link](https://github.com/NemoHoHaloAi/Competition/tree/master/kaggle/Topxxx-yyy-zzz-Large-Grocery-Chain-Predict)
        3. [start with](https://www.kaggle.com/ceshine/lgbm-starter)
    - [Store Item Demand Forecasting](https://www.kaggle.com/c/demand-forecasting-kernels-only)
        1. deathline: `2019-12-15`;
        2. [github project link](https://github.com/NemoHoHaloAi/Competition/tree/master/kaggle/Topxxx-yyy-zzz-Store-Item-Demand-Forecasting-Challenge)
        3. [notebook](https://www.kaggle.com/holoong9291/store-item-demand-predict)
    - [Online product sales](https://www.kaggle.com/c/online-sales/data)
        1. deathline: `2019-12-31`;
        2. [github project link](xxxx)
        3. [notebook](xxxx)
    - [Recruit Restaurant Visitor Forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)
        1. deathline: `2020-01-15`;
        2. [github project link](xxxx)
        3. [notebook](xxxx)
3. ![icon](https://img.shields.io/badge/todo|B-33%25-green) [summary](https://github.com/NemoHoHaloAi/Competition/tree/master/memo/Time-Series) of time-series projects(House-Price, Rossmann, Predict-Future-Sales)
    1. deathline: `2019-12-20`;
    2. target: can throw into the introduction;
4. ![icon](https://img.shields.io/badge/done-100%25-lightgrey) summary of time-series kernels(Time-Series projects)
    1. deathline: `2019-11-20`;
5. ![icon](https://img.shields.io/badge/todo-0%25-orange) summary of time-series(All about Time-Series)
    1. deathline: `2019-12-25`;
4. ![icon](https://img.shields.io/badge/todo-0%25-orange) update introduction with some project, and add detail in project
    1. deathline: `2019-12-31`;
    2. target: can give it to the interviewers and show my Time-Series skills;

## PLAN-jaguar ![icon](https://img.shields.io/badge/doing-15%25-green)

cycle time: from `2019-12-01` to `2019-12-31`;

0. [autoML](https://www.kaggle.com/practical-model-evaluation?utm_medium=email&utm_source=intercom&utm_campaign=model-evaluation-workshop)
1. Learn theoretical knowledge about ML
    1. ![icon](https://img.shields.io/badge/todo-0%25-orange) 李航《统计学习方法》+[code](https://github.com/WenDesi/lihang_book_algorithm)：建议边看书边手推公式然后对着GitHub一边敲代码;
    2. ![icon](https://img.shields.io/badge/doing|A-80%25-green) 林轩田《机器学习基石》+[video+link+note](https://github.com/NemoHoHaloAi/NTU-HsuanTienLin-MachineLearning/tree/master/Machine%20Learning%20Foundations);
    3. ![icon](https://img.shields.io/badge/todo-0%25-orange) 林轩田《机器学习技法》+[video+link+note](https://github.com/NemoHoHaloAi/NTU-HsuanTienLin-MachineLearning/tree/master/Machine%20Learning%20Techniques);
    4. ![icon](https://img.shields.io/badge/todo-0%25-orange) [线性代数和微积分](https://github.com/NemoHoHaloAi/OpenCourseCatalog);
    5. ![icon](https://img.shields.io/badge/todo-0%25-orange) [Spark in Python](http://spark.apache.org/docs/latest/api/python/index.html);

## DAILY MEMO

目前最需要的是什么？
1. 拿得出手的项目；
2. 实际项目中遇到的问题、解决方案、各项技能知识；
3. 基础知识；

同时进行的事：NFL、KernelLearn、基石课程；

- 2019-11-06: 完成Future-Sales-Predict项目从good-kernel上借鉴部分，目前完成了预处理、特征工程部分；
- 2019-11-07: 完成数据分割、baseline、遍历方式优化部分、数据转换（对比多种方式并分别应用到特征上）、数据转换无法降低skew的原因及处理方法、绘制整个过程中的数据结构变化；
- 2019-11-08: 数据转换后转换回来的问题、调整数据转换代码到合入test前并分别对matrix和train做转换、数据结构变化收尾；
- 2019-11-09: 基石-chapter2，基石-chapter3；
- 2019-11-10: 预测结果与实际结果对比、生成结果文件、计算分数(0.95)、模型堆叠、开放kernel并增加baseon和优化修改点，基石-chapter4，基石-chapter5，基石-chapter6；
- 2019-11-11: 离线运行Future-Sales-Predict、Kernel(基于TS数据能做的所有事)；
- 2019-11-12: Kernel(基于TS数据能做的所有事)-surplus；
- 2019-11-13: Kernel(Bitcoin-Predict-by-ARIMA)；
- 2019-11-14: Kernel(Bert NFL started Kernel)、NFL-Big-Data-Bowl(开始、数据加载、定义问题、评价函数、Preprocess)；
- 2019-11-15: NFL-Big-Data-Bowl(调研理解数据和问题、Preprocess)；
- 2019-11-16: NFL-Big-Data-Bowl(EDA、FE（FE部分暂时不考虑聚合的球员信息挖掘）)；
- 2019-11-17: NFL-Big-Data-Bowl(聚合后球员信息挖掘、算法替换为性能更好的，比如xgboost、模型堆叠集成)；
- 2019-11-18: NFL-Big-Data-Bowl(还有几个需要处理的特征：Location，PlayerCollegeName，Stadium，GameWeather，OffensePersonnel，DefensePersonnel，统一处理这一类问题，test中的类别不存在train中时如何处理、设计新的特征提交结果)；
- 2019-11-19: 修复一些bug，提交结果、score上看没有提升；
- 2019-11-20: 开始在22个球员的位置、速度、角度等信息上进行挖掘，得到的信息不足以提高模型的准确性；
- 2019-11-24: 开始考虑用简单特征、复杂模型来处理，基石-chapter7、8、9；
- 2019-11-25~27: 离线运行、分kernel验证、模型融合；
- 2019-12-02: NFL暂停等待竞赛结束后总结整理、Future-Sales-Predict暂停；
- 2019-12-08: Store-Item-Demand基础处理、ARIMA预测；
- 2019-12-09: Store-Item-Demand prophet预测、ARIMA参数3噪声的移动平均确定，为ARIMA加入网格搜索最优参数、xgboost第一次预测（尽可能做的少，直接预测结果，看rmse）、基石-chapter10；
- 2019-12-10: Store-Item-Demand ARIMA最优参数确定(目前只通过store==1&item==1的数据来进行网格搜索,711)、xgboost增加更多FE部分、基石-chapter11；
- 2019-12-11: Store-Item-Demand(拆分到3个notebook，内存优化，继续XGBoost版本特征工程)；
- 2019-12-12: Store-Item-Demand(XGBoost版本FE初版，三个版本生成对应的结果文件)；
- 2019-12-13: Store-Item-Demand(Prophet调参(初步调试后默认参数效果最好)，XGBoost继续优化)；
- 2019-12-16: Store-Item-Demand(提交对比实际分数，XGBoost-31.42702，XGBoost(NoFE)-25.21289，prophet-14.38283）、基石-12；
- 2019-12-17: Store-Item-Demand(XGBoost目前提交分数30+，远高于不做FE的提交结果，应该是有bug，修复问题优化分数，目前XGBoost-14.31967，已经在public,private上低于prophet，后续继续考虑优化(细化lag参数，目前是90,180,365，考虑细化，分析目前的模型复杂度与EinEout关系等，判断当前是欠拟合还是过拟合，验证集划分方式是否有更好的办法)，尽量到13.xxx)；
- 2019-12-18: Store-Item-Demand(分析FuturePricePredict中的处理方法(测试集、验证集都只是一个月的数据，因此计算是lag1也是有值的，从这个角度看，目前的lag90开始是没问题的，只不过因为步长过长，时间相关性没有那么强，这里要细化lag)，XGBoost参数优化，矫正因子，判断过欠拟合，目前XGBoost-14.16492)，基石-13，14；
- 2019-12-18：Store-Item-Demand(XGBoost调参)、基石-15、手写识别(绘制图片，增加旋转缩放等新数据hinting)；
