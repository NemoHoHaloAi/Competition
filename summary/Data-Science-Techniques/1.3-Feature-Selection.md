# 特征选择 - 了解特征并进行选择的过程

对于特征做选择，有助于减少过拟合问题以及加快模型速度，这在生产环境是很有意义的，一般来说会降低模型的性能，但是与速度平衡后选择一个合适的点是我们需要的；

## from [kernel](https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization?scriptVersionId=4107374)

关于该kernel：非常推荐的一篇关于特征选择的分享，不仅仅介绍了各种类型的代表方法，同时对于相关的可视化方法进行了展示，非常有用，感谢大佬们分享，不敢想象没有你们的日子；

1. 数据分析：
    1. 简单查看数据：
        1. id变量没法用于预测；
        2. Diagnosis是目标变量
        3. Unnamed: 32 包含大量NaN，直接丢弃；
        4. 对于其他字段的含义一无所知（这里没有上下文导致），但是特征选择这一步基本都是工程化的，不需要太多业务知识，当然知道更好；
    2. 对于线性模型、NN等，对数据做标准化或者规范化必要的，另一个好处是有助于特征在同样的坐标轴进行可视化；
2. 数据可视化：
    1. 首先进行标准化；
    2. 用[melt](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html)转换数据为id,变量名,变量值的方式太爽了；
    3. 通过violinplot图快速看出单个变量中哪些对分类结果会有好的性能，主要看分类的两边数据的分布位置，差异越大，提供信息越多；
    4. 箱行图也能提供类似的信息，差异主要在于箱行图更注重百分位数的位置，而小提琴图更注重数据整体分布情况；
    5. 两个变量相关性对比可以通过joint plot，多个变量可以通过pair plot；
  	6. 对比volin plot，swarmplot更直观的体现数据个体在某个维度上的分布情况；
    7. 使用heatmap对所有变量的相关性做可视化展示；
3. 特征选择：
    1. 各种方法：
        - 特征相关性
        - 单变量特征选择
        - 递归特征消除（RFE）
        - 交叉验证递归特征消除（RFECV）
        - 基于树的特征选择
    2. 特征相关性：
        1. 通过热图过滤相似特征：对于多个相似特征，保留在之前的swarm中看起来方差更清晰的特征，去掉其他；
        2. 这一步更多的是考虑对多于特征的去除，避免过拟合问题；
        3. 注意这一步的特点是没有考虑与目标特征的相关性，仅考虑自变量间的相关性进行选择；
        4. 因为只考虑了自变量间，所以选择标准要高，这个kernel中是只提出了相关系数为1的组；
    3. 单变量特征选择：
        1. 使用SelectKBest，指定score_func，对特征进行排序，选择Topk个；
        2. 注意这里是基于特征相关性去除后的16个剩余特征做的重要性排序，如果不是，那么相似的特征会得到相似的分数，这会影响结果，注意注意；
        3. 注意看混合矩阵，对比特征相关性那里，特征从16降为5个，在随机森林上的valid结果却提升了一点，这就是特征选择在小数据集上对过拟合的优化；
    4. 递归特征消除RFE：
        1. rfe = RFE(estimator=clf_rf, n_features_to_select=5, step=1)
        1. 基于模型(比如随机森林)，为每个特征赋予权重，权重最小的会被剔除，重复这个过程，直到特征数满足要求为止；
        2. 可想而知这个过程是非常耗时的，结果方面应该是很好的，因为这个选择的过程是基于模型的结果来做的；
        3. 通过RFE进行，看到设置5时，4个特征与单变量特征选择是一致的，相对来说这里的结果更可信，单变量可能存在特征间信息重合度高问题；
        4. 一个问题：为什么是5？？？
    5. 结合交叉验证的递归特征消除RFECV：
        1. rfecv = RFECV(estimator=clf_rf, cv=5, step=1, scoring='accuracy')
        1. 与RFE比，增加了CV部分，好处是可以自动选择合适的特征数量，不需要我们设置；
        2. 看到这个kernel中，最后合适的特征数为14，这跟5还是差的很远的；
    6. 基于树的特征选择：
        1. 类似随机森林等model都有feature_importance_属性可以查看特征重要性；
4. 特征提取：
    1. 在应用PCA前，对数据做标准化处理，使其更适合PCA处理；
    2. PCA有助于先压缩整合重组数据，再进行选择，区别在于它对特征做了融合，因为最后可能选择了3个，但是这已经不是原来数据的3个特征了，这是经过变换后带有主要变化方向，即信息，但是没有业务意义的3个新特征；
